{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# unit 8.0 - introduction to Reinforcement Learning (RL)\n",
        "\n",
        "## Learning in the real world\n",
        "\n",
        "Suppose we want to create the brain for a robot to help us in everyday tasks at home. The robot is an \"agent\" that needs to learn in an \"environment\". The robot will use sensory data to gather information about the status of the environment and of itself, and use actions to affect the environment, move. All of this happens in a loop, just like in the image below.\n",
        "\n",
        "![](images/RL-intro1.png)\n",
        "\n",
        "An agent collects information about the environment in the following modalities:\n",
        "- imaging: cameras that collect 2D images, 3D cameras that can perceive depth\n",
        "- sound: hearing sounds, speech\n",
        "- proprioception: sensing the position of the agent's degrees of freedom\n",
        "\n",
        "An environment can be:\n",
        "- fully observable: like a chess or go board\n",
        "- not fully-observable: like a 3D first person shooter game (FPS)\n",
        "\n",
        "Learning in the real world involves learning a sequence of steps that eventually lead to solving a task. For example, many chess actions (moves) are necessary to win the game. Winning the game may give us a reward signal (happiness? money?) that we can use to lean how to get to the final winning state. See an example sequence of actions below:\n",
        "\n",
        "$action(0) \\rightarrow action(1) ... \\rightarrow action(N) \\rightarrow reward$\n",
        "\n",
        "Before each action, the agent senses the environment and then produces an action.\n",
        "\n",
        "$action(t+1) = Brain-function( state(t), action(t), state(t-1), action(t-1)... )$\n",
        "\n",
        "How do we use neural network to learn in this context?\n",
        "\n",
        "In the field of RL, usually we use the following terminology:\n",
        "\n",
        "- action $a$\n",
        "- state $s$\n",
        "- reward $r$\n",
        "- brain function $F$, sometimes called \"policy\" is the robot brain function\n",
        "\n",
        "A neural network that works in RL need to perform the following: use the brain function to create an action plan (create a sequence of actions) after observing the environment, and of course maintaining a history of previous states, actions and rewards.\n",
        "\n",
        "$a(t+1), a(t+2), ... = F( s(t), s(t-1), ...,  a(t), a(t-1), ..., r(t), r(t-1), ..., )$\n",
        "\n",
        "The question now is to have a way to train a model to generate the right action sequence. One idea is to train a model with \"imitation learning\". This model can learn sequences of actions leading to a reward directly, if such sequences are available. Sequence of actions are only available in narrow sets of applications. Also learning by imitation does not involve exploration of new ideas or techniques, so the results are often limited by the training set. \n",
        "\n",
        "\n",
        "## Reinforcement Learning\n",
        "\n",
        "Reinforcement learning (RL) in the strict sense tries to learn a \"brain function\" model by trial and error in an online setting, or operating in the real-life loop of sensing and actuation. RL supposes we have an agent and an environment, and that the agent get access to a \"reward\" signal only. The agent has to learn to operate exploring and trying and maximizing the reward.\n",
        "\n",
        "As you can imagine, this is more difficult than imitation learning (which is a subset of supervised learning) but leads to more optimal solutions because the randomness of exploration can be used to achieve fine-tuned trajectories.\n",
        "\n",
        "\n",
        "## Q-learning\n",
        "\n",
        "Q-Learning is a form of RL where the algorithm tries to create a \"quality Q function\" of current and previous states and actions. The Q-function can be used to select an action that bring the agent closer to high reward state.\n",
        "\n",
        "Q-learning works with the following formula:\n",
        "\n",
        "$Q(S(t),A(t))←Q(S(t),A(t))+α[R(t+1)+γ*maxQ(S(t+1),a(t+1))−Q(S(t),A(t))]$\n",
        "\n",
        "- $Q$ is the quality function, $maxQ$ is the next state with the highest $Q$ value\n",
        "- $s, a, r$ are state, action, reward\n",
        "\n",
        "This formula creates a Q-table by iterating multiple times (trials) in the environment and slowly learning what sets of actions maximize reward at each step.\n",
        "\n",
        "### Example application of Q-learning\n",
        "\n",
        "Here is an example of Q-learning applied to a simple 3x3 grid world, where an agent learns to navigate from a start state (S) to a goal state (G) while avoiding obstacles (X).\n",
        "\n",
        "Assuming the following grid world:\n",
        "\n",
        "```\n",
        "| S |   |   |\n",
        "|   | X |   |\n",
        "|   |   | G |\n",
        "```\n",
        "\n",
        "S: Start\n",
        "X: Obstacle\n",
        "G: Goal\n",
        "\n",
        "The agent can move in four directions: up, down, left, and right. Actions that would move the agent into an obstacle or outside the grid are invalid.\n",
        "\n",
        "Here's how the Q-table might look during different iterations of Q-learning:\n",
        "\n",
        "Initial Q-table\n",
        "\n",
        "| State | Up | Down | Left | Right |\n",
        "|-------|----|------|------|-------|\n",
        "| (1,1) |  0 |   0  |   0  |   0   |\n",
        "| (1,2) |  0 |   0  |   0  |   0   |\n",
        "| (1,3) |  0 |   0  |   0  |   0   |\n",
        "| (2,1) |  0 |   0  |   0  |   0   |\n",
        "| (2,2) |  0 |   0  |   0  |   0   |\n",
        "| (2,3) |  0 |   0  |   0  |   0   |\n",
        "| (3,1) |  0 |   0  |   0  |   0   |\n",
        "| (3,2) |  0 |   0  |   0  |   0   |\n",
        "| (3,3) |  0 |   0  |   0  |   0   |\n",
        "\n",
        "\n",
        "After Several Iterations of Q-learning\n",
        "\n",
        "| State | Up | Down | Left | Right |\n",
        "|-------|----|------|------|-------|\n",
        "| (1,1) |  0 |  0.2 |  0.2 |   0.2 |\n",
        "| (1,2) | 0.2|    0 |  0.2 |   0.2 |\n",
        "| (1,3) | 0.2|  0.2 |    0 |   0.2 |\n",
        "| (2,1) |  0 |  0.2 |  0.2 |   0.2 |\n",
        "| (2,2) | 0.2|  0.2 |  0.2 |   0.2 |\n",
        "| (2,3) | 0.2|  0.2 |  0.2 |     0 |\n",
        "| (3,1) | 0.2|    0 |  0.2 |     0 |\n",
        "| (3,2) | 0.2|    0 |    0 |   0.2 |\n",
        "| (3,3) |  0 |    0 |  0.2 |   0.2 |\n",
        "\n",
        "\n",
        "\n",
        "Here, each cell in the Q-table represents the value of taking a particular action from the corresponding state. After multiple iterations of Q-learning, the agent learns the optimal actions to take in each state to reach the goal while avoiding obstacles. The values in the Q-table converge towards the optimal action-values.\n",
        "\n",
        "\n",
        "### Q table in neural networks:\n",
        "\n",
        "We can represent this Q-table as a neural network. In this case, we can use a fully connected neural network with one hidden layer. The input layer will have 4 neurons (corresponding to the four actions: Up, Down, Left, Right), the hidden layer can have any number of neurons, and the output layer will have 9 neurons (corresponding to the 9 states).\n",
        "\n",
        "Let's represent this neural network with 4 neurons in the input layer, 6 neurons in the hidden layer, and 9 neurons in the output layer (one for each state):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QNetwork(\n",
            "  (fc1): Linear(in_features=4, out_features=6, bias=True)\n",
            "  (fc2): Linear(in_features=6, out_features=9, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(4, 6)  # Input layer: 4 neurons, Hidden layer: 6 neurons\n",
        "        self.fc2 = nn.Linear(6, 9)  # Hidden layer: 6 neurons, Output layer: 9 neurons\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))  # Applying ReLU activation function in the hidden layer\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the network\n",
        "model = QNetwork()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The weights between the input layer and the hidden layer will be a 4×6 matrix, and the weights between the hidden layer and the output layer will be a 6×9 matrix. We can initialize these weights randomly and update them during the training process using back-propagation and gradient descent.\n",
        "\n",
        "This neural network will take the current state-action pair as input and output the corresponding Q-value for each state."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
