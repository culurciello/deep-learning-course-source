{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unit 1.2 - Binary net in PyTorch with manual weights\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/culurciello/deep-learning-course-source/blob/main/source/lectures/12-binary-net-pytorch-manual.ipynb)\n",
    "\n",
    "Now we will learn what is a \"proper\" way to define this super simple AND neural network in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d7fbcff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8eef052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Professional way to define AND model in pytorch:\n",
    "\n",
    "class Logic_net(nn.Module): \n",
    "    def __init__(self): \n",
    "        super(Logic_net, self).__init__()\n",
    "        self.l1 = nn.Linear(2, 1) # A simple layer with 2 inputs and 1 output\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.l1(x)\n",
    "        return (x > 0).float() # Returns 1 or 0 based on input\n",
    "\n",
    "model = Logic_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f50dec98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3250, -0.1958]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1146], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.l1.weight)\n",
    "print(model.l1.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98d747a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.4500, 0.4500]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.5000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# AND model weights:\n",
    "model.l1.weight = nn.Parameter(torch.Tensor([[0.45,0.45]]))\n",
    "model.l1.bias = nn.Parameter(torch.Tensor([-0.5]))\n",
    "\n",
    "print(model.l1.weight)\n",
    "print(model.l1.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9b49f081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: 0.0 0.0 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# AND model test:\n",
    "i1 = torch.Tensor([0,0])\n",
    "o1 = model.forward(i1)\n",
    "i2 = torch.Tensor([0,1])\n",
    "o2 = model(i2)\n",
    "i3 = torch.Tensor([1,0])\n",
    "o3 = model(i3)\n",
    "i4 = torch.Tensor([1,1])\n",
    "o4 = model(i4)\n",
    "\n",
    "print(\"out:\", o1.item(),o2.item(),o3.item(),o4.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "99b63a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: 0.0 0.0 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# Easier way to define AND model since we are using only single perceptron:\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 1, bias = True),\n",
    "    #torch.nn.ReLU()\n",
    ")\n",
    "\n",
    "# print(model)\n",
    "# print(model[0].weight)\n",
    "\n",
    "# a AND model weights:\n",
    "model[0].weight = nn.Parameter(torch.Tensor([[1,1]]))\n",
    "model[0].bias = nn.Parameter(torch.Tensor([-1.5]))\n",
    "\n",
    "# AND model test:\n",
    "i1 = torch.Tensor([0,0])\n",
    "o1 = (model(i1) > 0).float()\n",
    "i2 = torch.Tensor([0,1])\n",
    "o2 = (model(i2) > 0).float()\n",
    "i3 = torch.Tensor([1,0])\n",
    "o3 = (model(i3) > 0).float()\n",
    "i4 = torch.Tensor([1,1])\n",
    "o4 = (model(i4) > 0).float()\n",
    "\n",
    "print(\"out:\", o1.item(),o2.item(),o3.item(),o4.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdf9c52",
   "metadata": {},
   "source": [
    "## HOMEWORK\n",
    "\n",
    "Try to make the XOR or XNOR neural network on your own.\n",
    "\n",
    "Tip: think about decomposing the XOR function: XOR(x1,x2) = (x1 AND NOT x2) OR (NOT x1 AND x2)\n",
    "\n",
    "Can it be solved with 1 neuron?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "012af34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the XOR neural network\n",
    "class XORNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XORNet, self).__init__()\n",
    "        \n",
    "        # Define the layers\n",
    "        self.l1 = nn.Linear(2, 2)  # 2 inputs -> 2 hidden neurons\n",
    "        self.l2 = nn.Linear(2, 1) # 2 hidden neurons -> 1 output\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass: hidden layer -> activation -> output layer\n",
    "        x = torch.sigmoid(self.l1(x))  # Apply sigmoid activation in the hidden layer\n",
    "        x = torch.sigmoid(self.l2(x))  # Apply sigmoid activation in the output layer\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = XORNet()\n",
    "# Set pre-defined weights for the hidden layer\n",
    "model.l1.weight = nn.Parameter(torch.Tensor([[-8.0, -8.0], [-9.0, -9.0]]))  # Predefined weights\n",
    "model.l1.bias = nn.Parameter(torch.Tensor([11.0, 4.0]))  # Predefined biases\n",
    "\n",
    "# Set pre-defined weights for the output layer\n",
    "model.l2.weight = nn.Parameter(torch.Tensor([[13.0, -13.0]]))  # Predefined weights\n",
    "model.l2.bias = nn.Parameter(torch.Tensor([-6.0]))  # Predefined bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ccaf71d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 1.]])\n",
      "\n",
      "Predicted Outputs:\n",
      "tensor([[0.0031],\n",
      "        [0.9982],\n",
      "        [0.9982],\n",
      "        [0.0027]])\n",
      "Input: tensor([0., 0.]), Output: tensor([0.])\n",
      "Input: tensor([0., 1.]), Output: tensor([1.])\n",
      "Input: tensor([1., 0.]), Output: tensor([1.])\n",
      "Input: tensor([1., 1.]), Output: tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "# Example inputs for XOR (truth table)\n",
    "inputs = torch.Tensor([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "# Perform the forward pass\n",
    "outputs = model(inputs)\n",
    "\n",
    "# Print the results\n",
    "print(\"Inputs:\")\n",
    "print(inputs)\n",
    "print(\"\\nPredicted Outputs:\")\n",
    "print(outputs.detach())\n",
    "\n",
    "for i in range(len(inputs)):\n",
    "    print(f\"Input: {inputs[i]}, Output: {(outputs[i]> 0.5).float()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b4e453",
   "metadata": {},
   "source": [
    "How did we calculate these Weights and Biases??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
